---
layout: default
title: 3_01_로지스틱 회귀_개인연습
parent: AI 기초
grand_parent: AI
nav_order: 7
---

# 시나리오
- 무작위의 생선이 하나 들어있는 랜덤생선박스를 판매하려고 함
- 완전 무작위 판매가 아닌 선호하는 생선 확률을 보여주고 선택이 가능하도록
    ex) A박스에 도미가 있을 확률을 표시
- 박스에 들어있는 생선의 확률을 구해야 함

## 랜덤생선박스의 확률
- 생선의 종류 7가지
- 생선의 특성이 주어지면 7개 생선에 대한 확률을 구함
- 길이, 높이, 두께, 대각선길이를 특성으로 사용
- 이진분류(타겟이 두 종류)와 다중분류(타겟이 세 종류 이상)

## k-최근접 이웃 분류기 사용해보기


#### 데이터 준비


```python
from google.colab import drive
drive.mount('/content/drive')
```

    Mounted at /content/drive
    


```python
import pandas as pd

fish = pd.read_csv('/content/drive/MyDrive/NEW/1week/03_분류(연습)/fish_csv_data.csv')
fish.head()
```





  <div id="df-342abc12-e45f-4c16-be2f-b35602262b77" class="colab-df-container">
    <div>

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Species</th>
      <th>Weight</th>
      <th>Length</th>
      <th>Diagonal</th>
      <th>Height</th>
      <th>Width</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Bream</td>
      <td>242.0</td>
      <td>25.4</td>
      <td>30.0</td>
      <td>11.5200</td>
      <td>4.0200</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Bream</td>
      <td>290.0</td>
      <td>26.3</td>
      <td>31.2</td>
      <td>12.4800</td>
      <td>4.3056</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Bream</td>
      <td>340.0</td>
      <td>26.5</td>
      <td>31.1</td>
      <td>12.3778</td>
      <td>4.6961</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Bream</td>
      <td>363.0</td>
      <td>29.0</td>
      <td>33.5</td>
      <td>12.7300</td>
      <td>4.4555</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Bream</td>
      <td>430.0</td>
      <td>29.0</td>
      <td>34.0</td>
      <td>12.4440</td>
      <td>5.1340</td>
    </tr>
  </tbody>
</table>
</div>
    <div class="colab-df-buttons">

  <div class="colab-df-container">
    <button class="colab-df-convert" onclick="convertToInteractive('df-342abc12-e45f-4c16-be2f-b35602262b77')"
            title="Convert this dataframe to an interactive table."
            style="display:none;">

  <svg xmlns="http://www.w3.org/2000/svg" height="24px" viewBox="0 -960 960 960">
    <path d="M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z"/>
  </svg>
    </button>

  

   
  </div>


<div id="df-ddfa0bd4-4fd7-421b-83c7-d8485bfe0b3c">
  <button class="colab-df-quickchart" onclick="quickchart('df-ddfa0bd4-4fd7-421b-83c7-d8485bfe0b3c')"
            title="Suggest charts"
            style="display:none;">


  </button>






  </div>




#### 생선 종류 확인


```python
print(pd.unique(fish['Species']))
```

    ['Bream' 'Roach' 'Whitefish' 'Parkki' 'Perch' 'Pike' 'Smelt']
    

#### 분류에 사용할 훈련 데이터 열과 타겟데이터 열 선택


```python
fish[['Weight','Length','Diagonal','Height','Width']]
```





  <div id="df-0430c195-8743-42a0-862d-42cdc38f2904" class="colab-df-container">
    <div>

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Weight</th>
      <th>Length</th>
      <th>Diagonal</th>
      <th>Height</th>
      <th>Width</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>242.0</td>
      <td>25.4</td>
      <td>30.0</td>
      <td>11.5200</td>
      <td>4.0200</td>
    </tr>
    <tr>
      <th>1</th>
      <td>290.0</td>
      <td>26.3</td>
      <td>31.2</td>
      <td>12.4800</td>
      <td>4.3056</td>
    </tr>
    <tr>
      <th>2</th>
      <td>340.0</td>
      <td>26.5</td>
      <td>31.1</td>
      <td>12.3778</td>
      <td>4.6961</td>
    </tr>
    <tr>
      <th>3</th>
      <td>363.0</td>
      <td>29.0</td>
      <td>33.5</td>
      <td>12.7300</td>
      <td>4.4555</td>
    </tr>
    <tr>
      <th>4</th>
      <td>430.0</td>
      <td>29.0</td>
      <td>34.0</td>
      <td>12.4440</td>
      <td>5.1340</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>154</th>
      <td>12.2</td>
      <td>12.2</td>
      <td>13.4</td>
      <td>2.0904</td>
      <td>1.3936</td>
    </tr>
    <tr>
      <th>155</th>
      <td>13.4</td>
      <td>12.4</td>
      <td>13.5</td>
      <td>2.4300</td>
      <td>1.2690</td>
    </tr>
    <tr>
      <th>156</th>
      <td>12.2</td>
      <td>13.0</td>
      <td>13.8</td>
      <td>2.2770</td>
      <td>1.2558</td>
    </tr>
    <tr>
      <th>157</th>
      <td>19.7</td>
      <td>14.3</td>
      <td>15.2</td>
      <td>2.8728</td>
      <td>2.0672</td>
    </tr>
    <tr>
      <th>158</th>
      <td>19.9</td>
      <td>15.0</td>
      <td>16.2</td>
      <td>2.9322</td>
      <td>1.8792</td>
    </tr>
  </tbody>
</table>
<p>159 rows × 5 columns</p>
</div>
    <div class="colab-df-buttons">

  <div class="colab-df-container">
    <button class="colab-df-convert" onclick="convertToInteractive('df-0430c195-8743-42a0-862d-42cdc38f2904')"
            title="Convert this dataframe to an interactive table."
            style="display:none;">

  <svg xmlns="http://www.w3.org/2000/svg" height="24px" viewBox="0 -960 960 960">
    <path d="M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z"/>
  </svg>
    </button>

 
  </div>


<div id="df-adcfe997-d29f-4253-9b29-30b4aa82c0f0">
  <button class="colab-df-quickchart" onclick="quickchart('df-adcfe997-d29f-4253-9b29-30b4aa82c0f0')"
            title="Suggest charts"
            style="display:none;">


  </button>





  </div>





```python
fish_input = fish[['Weight','Length','Diagonal','Height','Width']].to_numpy()
fish_input[:5]
```




    array([[242.    ,  25.4   ,  30.    ,  11.52  ,   4.02  ],
           [290.    ,  26.3   ,  31.2   ,  12.48  ,   4.3056],
           [340.    ,  26.5   ,  31.1   ,  12.3778,   4.6961],
           [363.    ,  29.    ,  33.5   ,  12.73  ,   4.4555],
           [430.    ,  29.    ,  34.    ,  12.444 ,   5.134 ]])




```python
fish_target = fish['Species'].to_numpy()
fish_target[:5]
```




    array(['Bream', 'Bream', 'Bream', 'Bream', 'Bream'], dtype=object)



#### 훈련 데이터와 테스트 데이터 나누기


```python
from sklearn.model_selection import train_test_split

train_input, test_input, train_target, test_target = train_test_split(fish_input, fish_target, random_state=42, stratify=fish_target)
```


```python
print(train_input.shape,test_input.shape,train_target.shape,test_target.shape)
```

    (119, 5) (40, 5) (119,) (40,)
    

#### 훈련 데이터셋과 테스트 데이터셋 표준화
- StandardScaler 사용


```python
from sklearn.preprocessing import StandardScaler

ss =StandardScaler()
ss.fit(train_input)
train_scaled = ss.transform(train_input)
test_scaled = ss.transform(test_input)
```


```python
train_input[:5]
```




    array([[150.    ,  22.    ,  24.7   ,   5.8045,   3.7544],
           [250.    ,  27.5   ,  28.9   ,   7.2828,   4.5662],
           [430.    ,  29.    ,  34.    ,  12.444 ,   5.134 ],
           [925.    ,  39.5   ,  45.3   ,  18.7542,   6.7497],
           [110.    ,  21.    ,  22.5   ,   5.6925,   3.555 ]])




```python
train_scaled[:5]
```




    array([[-0.75628803, -0.66065677, -0.62357446, -0.78015159, -0.45043644],
           [-0.45991057, -0.1248453 , -0.24414603, -0.4293487 ,  0.03516919],
           [ 0.07356886,  0.0212851 ,  0.2165885 ,  0.79541208,  0.37481797],
           [ 1.54063728,  1.0441979 ,  1.23743166,  2.29283234,  1.34130358],
           [-0.87483902, -0.75807703, -0.82232269, -0.80672937, -0.5697143 ]])



#### k-최근접 이웃 모델 사용


```python
from sklearn.neighbors import KNeighborsClassifier

kn = KNeighborsClassifier(n_neighbors=3)
kn.fit(train_scaled, train_target)

print(kn.score(train_scaled, train_target))
print(kn.score(test_scaled, test_target))
```

    0.8823529411764706
    0.75
    

#### 타겟 클래스 확인
- 분류된 클래스 확인


```python
print(kn.classes_)
```

    ['Bream' 'Parkki' 'Perch' 'Pike' 'Roach' 'Smelt' 'Whitefish']
    

'Bream': 도미
'Parkki' : 파키
'Perch' : 농어
'Pike' : 강꼬치고기
'Roach' : 로치
'Smelt' : 바다빙어
'Whitefish' : 백어

#### 분류 예측


```python
print(kn.predict(test_scaled[:5]))
```

    ['Perch' 'Perch' 'Roach' 'Parkki' 'Parkki']
    

#### 분류기의 예측 확률 확인


```python
import numpy as np

proba = kn.predict_proba(test_scaled[:5])
print(kn.classes_)
print(np.round(proba, decimals=4))
```

    ['Bream' 'Parkki' 'Perch' 'Pike' 'Roach' 'Smelt' 'Whitefish']
    [[0.     0.     0.6667 0.     0.3333 0.     0.    ]
     [0.     0.     0.6667 0.     0.3333 0.     0.    ]
     [0.     0.     0.3333 0.     0.6667 0.     0.    ]
     [0.     1.     0.     0.     0.     0.     0.    ]
     [0.     0.6667 0.     0.     0.3333 0.     0.    ]]
    


```python
distances, indexes = kn.kneighbors(test_scaled[3:4])
print(train_target[indexes])
```

    [['Parkki' 'Parkki' 'Parkki']]
    

##### 결과
- kneighbors()메서드는 2차원 배열을 전달해야 함(슬라이싱 하면 2차원 반환)
- 테스트 데이터셋의 1번째 샘플을 예측
- Roach가 하나(30%) Perch가 둘(60%)로 Perch라고 분류함
- 확률은 이웃의 개수로 만들어짐
- 예제는 3개의 이웃을 사용하므로 네 가지 확률이 나옴
    - 0/3, 1/3, 2/3, 3/3
- 확률이라고 하기에는 다소 아쉬움

# 로지스틱 회귀(Logistic Regression)
- 이름은 회귀이나 분류 모델임
- 선형 회귀와 동일한 선형 방정식으로 학습
    - $z = a \times (weight) + b \times (length) + c \times (diagonal) + d \times (height) + e \times (width) + f$
- a, b, c, d는 가중치 또는 계수(다중 회귀를 위한 선형 방정식과 동일)
- z는 어떤 값이라도 상관 없지만 확률이 되려면 0 ~ 1 (0 ~ 100%)
- 시그모이드 함수를 활용
    - z가 아주 큰 음수면 0, 아주 큰 양수면 1을 사용하도록 만드는 것
    - $\varnothing = {1 \over 1+{e^{-z}}}$
    - 자연상수 $e$를 선형 방정식의 출력 $z$의 음수로 거듭제곱하고 1을 더한 값의 역수를 취함
    - 다음과 같은 그래프를 만들 수 있음

    


```python
import numpy as np
import matplotlib.pyplot as plt

z = np.arange(-5,5,0.1) # -5 ~ 5 까지 값을 0.1단위로 생성
z
```




    array([-5.00000000e+00, -4.90000000e+00, -4.80000000e+00, -4.70000000e+00,
           -4.60000000e+00, -4.50000000e+00, -4.40000000e+00, -4.30000000e+00,
           -4.20000000e+00, -4.10000000e+00, -4.00000000e+00, -3.90000000e+00,
           -3.80000000e+00, -3.70000000e+00, -3.60000000e+00, -3.50000000e+00,
           -3.40000000e+00, -3.30000000e+00, -3.20000000e+00, -3.10000000e+00,
           -3.00000000e+00, -2.90000000e+00, -2.80000000e+00, -2.70000000e+00,
           -2.60000000e+00, -2.50000000e+00, -2.40000000e+00, -2.30000000e+00,
           -2.20000000e+00, -2.10000000e+00, -2.00000000e+00, -1.90000000e+00,
           -1.80000000e+00, -1.70000000e+00, -1.60000000e+00, -1.50000000e+00,
           -1.40000000e+00, -1.30000000e+00, -1.20000000e+00, -1.10000000e+00,
           -1.00000000e+00, -9.00000000e-01, -8.00000000e-01, -7.00000000e-01,
           -6.00000000e-01, -5.00000000e-01, -4.00000000e-01, -3.00000000e-01,
           -2.00000000e-01, -1.00000000e-01, -1.77635684e-14,  1.00000000e-01,
            2.00000000e-01,  3.00000000e-01,  4.00000000e-01,  5.00000000e-01,
            6.00000000e-01,  7.00000000e-01,  8.00000000e-01,  9.00000000e-01,
            1.00000000e+00,  1.10000000e+00,  1.20000000e+00,  1.30000000e+00,
            1.40000000e+00,  1.50000000e+00,  1.60000000e+00,  1.70000000e+00,
            1.80000000e+00,  1.90000000e+00,  2.00000000e+00,  2.10000000e+00,
            2.20000000e+00,  2.30000000e+00,  2.40000000e+00,  2.50000000e+00,
            2.60000000e+00,  2.70000000e+00,  2.80000000e+00,  2.90000000e+00,
            3.00000000e+00,  3.10000000e+00,  3.20000000e+00,  3.30000000e+00,
            3.40000000e+00,  3.50000000e+00,  3.60000000e+00,  3.70000000e+00,
            3.80000000e+00,  3.90000000e+00,  4.00000000e+00,  4.10000000e+00,
            4.20000000e+00,  4.30000000e+00,  4.40000000e+00,  4.50000000e+00,
            4.60000000e+00,  4.70000000e+00,  4.80000000e+00,  4.90000000e+00])




```python
phi = 1 / (1 + np.exp(-z))

plt.plot(z, phi)
plt.xlabel('z')
plt.ylabel('phi')
plt.show()
```


    
<img src = "/assets/images/1week/3_01_로지스틱 회귀_개인연습_files/3_01_로지스틱 회귀_개인연습_31_0.png"> 

##### 결과
- -5 ~ 5까지의 0.1단위의 값을 0 ~ 1 사이로 표현함(가둔다고 표현)

## 로지스틱 회귀로 이진 분류 하기


```python
char_arr = np.array(['A', 'B', 'C', 'D', 'E'])
print(char_arr[[True, False, True, False, False]])
```

    ['A' 'C']
    


```python
train_target
```




    array(['Roach', 'Perch', 'Bream', 'Bream', 'Perch', 'Perch', 'Perch',
           'Bream', 'Whitefish', 'Bream', 'Perch', 'Bream', 'Perch', 'Smelt',
           'Perch', 'Roach', 'Smelt', 'Perch', 'Perch', 'Bream', 'Bream',
           'Perch', 'Smelt', 'Roach', 'Bream', 'Parkki', 'Bream', 'Perch',
           'Perch', 'Smelt', 'Bream', 'Whitefish', 'Roach', 'Pike', 'Perch',
           'Pike', 'Smelt', 'Perch', 'Bream', 'Perch', 'Parkki', 'Perch',
           'Perch', 'Roach', 'Parkki', 'Bream', 'Roach', 'Bream', 'Perch',
           'Parkki', 'Bream', 'Smelt', 'Perch', 'Bream', 'Bream', 'Pike',
           'Bream', 'Perch', 'Perch', 'Smelt', 'Pike', 'Bream', 'Pike',
           'Perch', 'Perch', 'Bream', 'Roach', 'Perch', 'Roach', 'Perch',
           'Pike', 'Perch', 'Roach', 'Bream', 'Perch', 'Bream', 'Pike',
           'Perch', 'Pike', 'Perch', 'Parkki', 'Whitefish', 'Pike', 'Bream',
           'Smelt', 'Perch', 'Perch', 'Roach', 'Bream', 'Roach', 'Perch',
           'Perch', 'Bream', 'Pike', 'Roach', 'Parkki', 'Bream', 'Roach',
           'Roach', 'Pike', 'Roach', 'Perch', 'Smelt', 'Parkki', 'Pike',
           'Perch', 'Smelt', 'Whitefish', 'Bream', 'Perch', 'Perch', 'Perch',
           'Perch', 'Perch', 'Parkki', 'Pike', 'Whitefish', 'Perch', 'Perch'],
          dtype=object)




```python
train_target == 'Bream' # 도미인 데이터 확인
```




    array([False, False,  True,  True, False, False, False,  True, False,
            True, False,  True, False, False, False, False, False, False,
           False,  True,  True, False, False, False,  True, False,  True,
           False, False, False,  True, False, False, False, False, False,
           False, False,  True, False, False, False, False, False, False,
            True, False,  True, False, False,  True, False, False,  True,
            True, False,  True, False, False, False, False,  True, False,
           False, False,  True, False, False, False, False, False, False,
           False,  True, False,  True, False, False, False, False, False,
           False, False,  True, False, False, False, False,  True, False,
           False, False,  True, False, False, False,  True, False, False,
           False, False, False, False, False, False, False, False, False,
            True, False, False, False, False, False, False, False, False,
           False, False])




```python
train_target == ' Smelt' # 빙어인 데이터 확인
```




    array([False, False, False, False, False, False, False, False, False,
           False, False, False, False, False, False, False, False, False,
           False, False, False, False, False, False, False, False, False,
           False, False, False, False, False, False, False, False, False,
           False, False, False, False, False, False, False, False, False,
           False, False, False, False, False, False, False, False, False,
           False, False, False, False, False, False, False, False, False,
           False, False, False, False, False, False, False, False, False,
           False, False, False, False, False, False, False, False, False,
           False, False, False, False, False, False, False, False, False,
           False, False, False, False, False, False, False, False, False,
           False, False, False, False, False, False, False, False, False,
           False, False, False, False, False, False, False, False, False,
           False, False])




```python
bream_smelt_indexes = (train_target == 'Bream') | (train_target == 'Smelt') # 두 가지만 고름
bream_smelt_indexes
```




    array([False, False,  True,  True, False, False, False,  True, False,
            True, False,  True, False,  True, False, False,  True, False,
           False,  True,  True, False,  True, False,  True, False,  True,
           False, False,  True,  True, False, False, False, False, False,
            True, False,  True, False, False, False, False, False, False,
            True, False,  True, False, False,  True,  True, False,  True,
            True, False,  True, False, False,  True, False,  True, False,
           False, False,  True, False, False, False, False, False, False,
           False,  True, False,  True, False, False, False, False, False,
           False, False,  True,  True, False, False, False,  True, False,
           False, False,  True, False, False, False,  True, False, False,
           False, False, False,  True, False, False, False,  True, False,
            True, False, False, False, False, False, False, False, False,
           False, False])




```python
bream_smelt_indexes = (train_target == 'Bream') | (train_target == 'Smelt') # 두 가지만 고름
train_bream_smelt = train_scaled[bream_smelt_indexes]
target_bream_smelt = train_target[bream_smelt_indexes]
```

### LogisticRegression 모델로 이진 분류 하기


```python
from sklearn.linear_model import LogisticRegression

lr = LogisticRegression()
lr.fit(train_bream_smelt, target_bream_smelt)
```







```python
# 예측하기
print(lr.predict(train_bream_smelt[:5]))
```

    ['Bream' 'Bream' 'Bream' 'Bream' 'Bream']
    


```python
# 퍼센트 확인
print(lr.predict_proba(train_bream_smelt[:5]))
```

    [[9.76263188e-01 2.37368125e-02]
     [9.99614480e-01 3.85519926e-04]
     [9.94438266e-01 5.56173414e-03]
     [9.09188993e-01 9.08110069e-02]
     [9.99361521e-01 6.38479331e-04]]
    


```python
# 분류 클래스 확인
print(lr.classes_)
```

    ['Bream' 'Smelt']
    


```python
# 회귀 계수 확인
print(lr.coef_,lr.intercept_)
```

    [[-0.4235112  -0.61604834 -0.70216369 -0.97498265 -0.7403996 ]] [-2.46732659]
    

##### 방정식 좀 바꿔서 다름
$z = -0.404 \times (weight) + -0.576 \times (length) + -0.662 \times (diagonal) + -1.012 \times (height) + -0.731 \times (width) + -2.161$


```python
# z값 확인
decision = lr.decision_function(train_bream_smelt[:5])
print(decision)
```

    [-3.7167051  -7.86053208 -5.18626807 -2.30377249 -7.35578257]
    


```python
# 시그모이드 함수에 적용하면 확률을 구할 수 있음
from scipy.special import expit # 시그모이드 함수

print(expit(decision))
```

    [0.02373681 0.00038552 0.00556173 0.09081101 0.00063848]
    

##### 결과
- decision_function() 메서드는 양성인 클래스에 대한 z값 확인 가능

### LogisticRegression 모델로 다중 분류 하기
- 기본적으로 반복적인 알고리즘을 사용
- max_iter옵션으로 반복 횟수 지정(기본값: 100)
- 릿지 회귀와 같이 계수의 제곱을 규제(L2규제)
- 규제 제어 매개변수 C(기본값: 1)이며 alpha와 반대 개념으로 적용(값이 작으면 규제가 커짐)


```python
# 훈련하기
from sklearn.linear_model import LogisticRegression

lr = LogisticRegression(C=20, max_iter=1000)
lr.fit(train_scaled, train_target)

print(lr.score(train_scaled,train_target))
print(lr.score(test_scaled,test_target))
```

    0.9243697478991597
    0.975
    

##### 결과
- 훈련 데이터셋과 테스트 데이터셋에 대한 점수가 높음
- 과대적합이나 과소적합은 아닌 것으로 보임

- 흠.. train_test_split 할때  stratify=fish_target이거 하나 넣어줬는데 많이바뀌넹


```python
# 예측
lr.predict(test_scaled[:5])
```




    array(['Roach', 'Perch', 'Perch', 'Parkki', 'Parkki'], dtype=object)




```python
# 퍼센트 확인
proba = lr.predict_proba(test_scaled[:5])
print(lr.classes_)
print(np.round(proba, decimals=3))
```

    ['Bream' 'Parkki' 'Perch' 'Pike' 'Roach' 'Smelt' 'Whitefish']
    [[0.    0.029 0.237 0.003 0.685 0.01  0.035]
     [0.    0.032 0.576 0.001 0.35  0.003 0.039]
     [0.    0.062 0.558 0.001 0.336 0.017 0.026]
     [0.003 0.93  0.001 0.    0.051 0.    0.015]
     [0.001 0.882 0.004 0.    0.094 0.002 0.017]]
    

##### 해석
- 첫 번째 열이 음성 클래스(0)에 대한 확률
- 두 번째 열은 양성 클래스(1)에 대한 확률
- 분류된 클래스들은 타겟값을 알파벳 순으로 정렬하여 사용(classes_ 속성으로 확인)


```python
print(lr.classes_)
```

    ['Bream' 'Parkki' 'Perch' 'Pike' 'Roach' 'Smelt' 'Whitefish']
    


```python
print(lr.coef_.shape,lr.intercept_.shape)
```

    (7, 5) (7,)
    

##### 해석
- 5개의 특성을 사용했으므로 열이 5개 이다.
- 분류를 위한 행은 7개 이다.
- 각각의 클래스마다의 확률을 모두 계산
- 이중 분류는 시그모이드를 적용하고, 다중 분류는 소프트맥스 함수를 사용함

#### 소프트맥스(softmax) 함수
- 시그모이드 함수는 하나의 선형 방정식의 출력 값을 0~1로 압축
- 소프트맥스 함수는 여러 개의 선형 방정식의 출력값을 0~1로 압축하고 전체 합이 1이 되도록 만듦
- 지수함수를 사용하므로 정규화된 지수함수라고도 불림

-
- $z1$ \~ $z7$ 까지 값을 사용하여 지수함수($e^{z1}$ \~ $e^{z7}$)을 계산하여 합을 구함(e_sum)
    - $ e\_sum = e^{z1} + e^{z2} + e^{z3} + e^{z4} + e^{z5} + e^{z6} + e^{z7}$
- $e^{z1}$ \~ $e^{z7}$을 각각 $e\_sum$으로 나눔
    - $ s1 = { e^{z1} \over e\_sum}, s2 = { e^{z2} \over e\_sum}, ... s7 = { e^{z7} \over e\_sum}$
- 이렇게 나온 $e^{s1}$ \~ $e^{s7}$을 모두 더하면 1이 된다.(7개 생선에 대한 확률의 합은 1이어야 함)


```python
# 훈련 데이터의 첫 번째 샘플에 대한 확률 값을 이용하여 1이 되는지 확인
0 + 0.014 + 0.841 + 0 + 0.136 + 0.007 + 0.003
```




    1.001



#### decision_function()메서드로 직접 해보기


```python
# 테스트 데이터셋의 처음 5개 샘플에 대한 z1~z7 값 구하기
decision = lr.decision_function(test_scaled[:5])
print(np.round(decision, decimals=2))
```

    [[-4.55  0.4   2.49 -1.74  3.55 -0.71  0.57]
     [-4.46  0.79  3.68 -2.48  3.18 -1.69  0.98]
     [-5.74  1.39  3.59 -2.95  3.08  0.09  0.53]
     [-0.35  5.41 -1.5  -4.69  2.51 -2.66  1.27]
     [-2.16  4.86 -0.59 -4.54  2.62 -1.1   0.9 ]]
    


```python
# softmax함수로 확률 확인
from scipy.special import softmax

proba = softmax(decision, axis=1)
print(np.round(proba, decimals=3))
```

    [[0.    0.029 0.237 0.003 0.685 0.01  0.035]
     [0.    0.032 0.576 0.001 0.35  0.003 0.039]
     [0.    0.062 0.558 0.001 0.336 0.017 0.026]
     [0.003 0.93  0.001 0.    0.051 0.    0.015]
     [0.001 0.882 0.004 0.    0.094 0.002 0.017]]
    


```python
# predict_proba와 비교
proba = lr.predict_proba(test_scaled[:5])
print(np.round(proba, decimals=3))
```

    [[0.    0.029 0.237 0.003 0.685 0.01  0.035]
     [0.    0.032 0.576 0.001 0.35  0.003 0.039]
     [0.    0.062 0.558 0.001 0.336 0.017 0.026]
     [0.003 0.93  0.001 0.    0.051 0.    0.015]
     [0.001 0.882 0.004 0.    0.094 0.002 0.017]]
    

##### 결과
- 로지스틱 회귀 모델이 구한 확률과 동일한
- 로지스틱 회귀로 이중, 다중 분류를 수행할 수 있음

## 정리하기

### 로지스틱 회귀
- 선형 방정식을 사용한 분류 알고리즘
- 선형 회귀와 달리 시그모이드 함수 또는 소프트맥스 함수를 사용하여 클래스 확률 출력 가능

### 다중 분류
- 분류할 타겟 클래스가 3개 이상인 분류 문제
- 로지스틱 회귀는 다중 분류를 위해 softmax함수를 사용

### 시그모이드 함수
- 선형 방정식의 출력을 0과 1사이의 값으로 압축
- 이진 분류를 위해 사용

### 소프트맥스 함수
- 다중 분류 시 여러 타겟 클래스에 대한 선형 방정식의 출력 결과를 정규화
- 정규화된 값의 총 합이 1이 되도록 하여 확률을 구함
