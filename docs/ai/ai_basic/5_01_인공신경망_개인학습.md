```python
# !pip install tensorflow

# 실행마다 동일한 결과를 얻기 위해 케라스에 랜덤 시드를 사용하고 텐서플로 연산을 결정적으로 만듭니다.
import tensorflow as tf

tf.keras.utils.set_random_seed(42)
tf.config.experimental.enable_op_determinism()
```

#### 데이터 준비
- 케라스에서 제공하는 MNIST패션 데이터 로딩
- 훈련세트와 테스트세트를 나누어 반환하므로 그에 맞춰서 대입
- train_test_split과 결과가 다르니 주의


```python
from tensorflow import keras
(X_train, y_train), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()
```


```python
X_train.shape, y_train.shape
```




    ((60000, 28, 28), (60000,))




```python
X_test.shape, y_test.shape
```




    ((10000, 28, 28), (10000,))



##### 확인
- 총 60000개의 이미지 데이터셋
- 각 이미지는 28x28 크기
- 테스트 세트는 동일한 형태로 10000개의 이미지가 존재


```python
import matplotlib.pyplot as plt

fig, axs = plt.subplots(2,10,figsize=(10,3))
for i in range(20):
    row = i // 10  # 행 번호 계산
    col = i % 10   # 열 번호 계산
    axs[row, col].imshow(X_train[i], cmap='gray_r')
    axs[row, col].axis('off')
plt.show()
```


    
![png](5_01_%EC%9D%B8%EA%B3%B5%EC%8B%A0%EA%B2%BD%EB%A7%9D_%EA%B0%9C%EC%9D%B8%ED%95%99%EC%8A%B5_files/5_01_%EC%9D%B8%EA%B3%B5%EC%8B%A0%EA%B2%BD%EB%A7%9D_%EA%B0%9C%EC%9D%B8%ED%95%99%EC%8A%B5_6_0.png)
    


#### 샘플들의 타겟 값 확인
0: 티셔츠  
1: 바지  
2: 스웨터  
3: 드레스  
4: 코트  
5: 샌들  
6: 셔츠  
7: 스니커즈  
8: 가방  
9: 앵클 부츠  


```python
[y_train[i] for i in range(10)]
```




    [9, 0, 0, 3, 0, 2, 7, 2, 5, 5]




```python
import numpy as np

np.unique(y_train,return_counts=True)
```




    (array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8),
     array([6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000]))



- 각 아이템마다 6000개의 샘플이 들어있음을 확인

### 로지스틱 회귀를 이용하여 패션 아이템 분류하기
- 훈련 샘플의 개수가 많으므로 한 번에 훈련하는 것보다 샘플을 하나씩 이용하여 훈련하는 것이 효율적으로 보임
- 확률적 경사 하강법을 이용하는 것이 알맞아 보임
- SGDClassifier를 이용
    - 확률적 경사 하강법은 여러 특성 중 기울기가 가장 가파른 방향으로 이동함
    - 특성마다 범위가 다르면 손실 함수의 경사를 내려오는 것이 문제가 생길 수 있음
    - 현재 데이터는 각 픽셀이 0 ~ 255 사이의 정수값을 가지므로 255로 나누어 0~1사이의 값으로 정규화 적용
    - 표준화는 아니지만 양수 값으로 이루어진 데이터를 전처리할 때 많이 활용하는 기법

아니! 표준화도 해서 비교해본다!


```python
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

X_train_scaled1 = scaler.fit_transform(X_train.reshape(-1,28*28))
X_test_scaled1 = scaler.fit_transform(X_test.reshape(-1,28*28))
```


```python
X_train_scaled1[0]
```




    array([-8.64371176e-03, -2.32233257e-02, -3.91780598e-02, -4.13217164e-02,
           -5.76457447e-02, -7.11673332e-02, -9.88784561e-02, -1.56652590e-01,
           -2.39080340e-01, -3.77827057e-01, -5.70545271e-01, -6.94039140e-01,
           -7.58868529e-01, -7.48857821e-01, -7.49009030e-01, -7.75175617e-01,
           -7.57573332e-01, -6.46914032e-01, -5.04399791e-01, -3.19932035e-01,
           -2.03351232e-01, -1.40220976e-01, -1.09811552e-01, -9.22242465e-02,
           -6.82766692e-02, -5.05167826e-02, -3.32743014e-02, -1.36223522e-02,
           -1.25721114e-02, -1.78614038e-02, -3.30493250e-02, -5.59065175e-02,
           -7.80937469e-02, -1.31089676e-01, -2.43799305e-01, -3.63949871e-01,
           -4.88007101e-01, -6.43607971e-01, -8.34050515e-01, -1.00280952e+00,
           -1.09141822e+00, -1.08265217e+00, -1.06645189e+00, -1.10652482e+00,
           -1.08951320e+00, -9.40120261e-01, -7.80195032e-01, -5.94466180e-01,
           -4.41415891e-01, -3.16307390e-01, -2.07437016e-01, -1.48946980e-01,
           -1.18545275e-01, -9.00584471e-02, -5.84777596e-02, -2.89463605e-02,
           -1.55250102e-02, -2.55016438e-02, -4.60335891e-02, -7.36947876e-02,
           -1.28855737e-01, -2.69055198e-01, -4.03767067e-01, -5.02407714e-01,
           -6.29941706e-01, -8.22138132e-01, -9.94090278e-01, -1.07168098e+00,
           -1.10099112e+00, -1.10018820e+00, -1.09260008e+00, -1.12143088e+00,
           -1.11731206e+00, -1.08166173e+00, -9.77659395e-01, -7.77347271e-01,
           -5.93383209e-01, -4.82721692e-01, -3.71855993e-01, -2.40361456e-01,
           -1.69903541e-01, -1.29873524e-01, -8.77680606e-02, -5.10419372e-02,
           -2.52882311e-02, -4.09018449e-02, -6.61575758e-02, -1.03739726e-01,
           -2.14586361e-01, -3.70967737e-01, -5.06444224e-01, -6.38254000e-01,
           -7.63104798e-01, -9.34099241e-01, -1.05096037e+00, -1.09578142e+00,
           -1.09536996e+00, -1.10459738e+00, -1.11272655e+00, -9.97553536e-01,
           -3.62291073e-01, -1.14198004e+00, -1.09122820e+00, -9.32134300e-01,
           -7.10055765e-01, -6.18609465e-01, -4.87159118e-01, -3.48865521e-01,
           -2.34059507e-01, -1.35637172e-01, -6.38090130e-02, -7.17806116e-02,
           -3.87036030e-02, -6.23677128e-02, -9.13416395e-02, -1.49972810e-01,
           -2.88948287e-01, -4.55010302e-01, -6.10019806e-01, -7.20011451e-01,
           -8.19599736e-01, -9.80921818e-01, -1.07852132e+00, -1.11140327e+00,
           -1.08442226e+00, -1.12688198e+00, -7.67314414e-01,  2.88881062e-01,
            1.91328569e-01, -5.14218273e-01, -5.68149230e-01, -1.03749725e+00,
           -8.57112673e-01, -7.40730489e-01, -5.90499984e-01, -3.94260305e-01,
           -2.25226702e-01, -2.28202710e-01, -1.63194705e-01,  2.26936583e-01,
           -5.27738507e-02, -8.55347437e-02, -1.23205639e-01, -1.99884081e-01,
           -3.49441763e-01, -5.32773576e-01, -6.73940039e-01, -7.71010098e-01,
           -8.67241951e-01, -1.02510990e+00, -1.10665175e+00, -1.13128628e+00,
           -1.06763347e+00, -1.15820469e+00, -9.06770091e-02,  1.01045999e+00,
            6.99522095e-01,  2.25794987e-01,  3.61894274e-01,  2.45283325e-01,
           -6.88458737e-01, -8.30798228e-01, -7.05266006e-01, -5.33291732e-01,
           -3.89207236e-01, -2.88723154e-02,  9.70067710e-02, -1.24993840e-01,
           -6.74766339e-02, -1.07827085e-01, -1.54699010e-01, -2.45530038e-01,
           -4.00839526e-01, -5.89066590e-01, -7.19595407e-01, -8.12977617e-01,
           -8.97880010e-01, -1.05170677e+00, -1.12900065e+00, -1.15556855e+00,
           -1.15971068e+00, -1.19578601e+00,  4.54462282e-01,  1.33017908e+00,
            1.00512266e+00,  6.71472085e-01, -1.14799402e-01,  5.39947300e-01,
            7.40207424e-01,  2.81313111e-01, -6.21859556e-02, -3.21856142e-01,
            6.91959891e-01,  2.01131756e+00,  1.58744438e+00,  8.77217815e-01,
           -8.52236311e-02, -1.30664694e-01, -1.84556908e-01, -2.85148564e-01,
           -4.42098980e-01, -6.30695495e-01, -7.53415593e-01, -8.44620043e-01,
           -9.22804074e-01, -1.06457195e+00, -1.14641003e+00, -1.16920386e+00,
           -1.18880496e+00, -4.68974036e-01,  1.02166685e+00,  1.13874661e+00,
            1.08443203e+00,  1.06831877e+00,  1.09485794e+00,  5.55522838e-01,
            2.74800881e-01,  3.07341636e-01,  4.76604173e-01,  1.06924551e+00,
            1.38818070e+00,  9.70315171e-01,  3.56044284e+00,  3.78754064e+00,
           -9.64995958e-02, -1.49604597e-01, -2.10817832e-01, -3.15498738e-01,
           -4.72229923e-01, -6.61456022e-01, -7.75266035e-01, -8.65736102e-01,
           -9.39610827e-01, -1.05928639e+00, -1.14859476e+00, -1.19434168e+00,
           -1.22440729e+00,  9.68781696e-01,  1.33930691e+00,  1.19697846e+00,
            1.22060342e+00,  1.18256381e+00,  1.13894149e+00,  1.18715938e+00,
            1.12622191e+00,  1.18675313e+00,  8.27357079e-01,  6.99178675e-01,
            9.69687327e-01,  2.31954860e+00,  4.20152607e+00, -1.86392546e-01,
           -1.06487096e-01, -1.66430000e-01, -2.28855076e-01, -3.35654105e-01,
           -4.93622358e-01, -6.81323855e-01, -7.87888968e-01, -8.77287678e-01,
           -9.52292365e-01, -1.07358657e+00, -1.17964225e+00, -1.23831928e+00,
           -1.27414629e+00,  7.41303525e-01,  1.24420035e+00,  9.41432990e-01,
            1.03640433e+00,  1.12084463e+00,  1.23746394e+00,  1.19612885e+00,
            1.15200889e+00,  1.21504737e+00,  1.39062920e+00,  1.65316931e+00,
            2.04625076e+00,  2.67673012e+00,  2.67822749e+00, -2.14850728e-01,
           -1.16649481e-01, -1.79923212e-01, -2.37131555e-01, -3.40659170e-01,
           -5.04079490e-01, -6.86741698e-01, -7.86984599e-01, -8.76341319e-01,
           -9.57328506e-01, -1.07795582e+00, -1.22103441e+00, -1.30801365e+00,
           -1.39946445e+00,  7.92184281e-01,  1.21605337e+00,  9.11657007e-01,
            8.62820370e-01,  6.69227025e-01,  5.05877344e-01,  9.95382616e-01,
            9.69854706e-01,  1.06808127e+00,  1.22141323e+00,  1.58463683e+00,
            1.87511299e+00,  2.37984329e+00,  2.72972119e+00, -2.41070420e-01,
           -1.30075447e-01, -1.92167591e-01, -2.38306697e-01, -3.35310376e-01,
           -4.99303339e-01, -6.77774191e-01, -7.75950888e-01, -8.72015558e-01,
           -9.75064948e-01, -1.10741105e+00, -1.30153112e+00, -1.47599773e+00,
           -1.45934872e+00,  1.05756138e+00,  1.08478123e+00,  8.20435373e-01,
            9.13500923e-01,  5.52885412e-01,  3.05633135e-01,  1.13130654e+00,
            9.16705095e-01,  1.11439206e+00,  1.29714193e+00,  1.39200124e+00,
            1.86328272e+00,  1.67451132e+00,  2.45960666e+00,  1.72376912e+00,
           -1.44276412e-01, -2.03649637e-01, -2.45893608e-01, -3.38602854e-01,
           -5.04339366e-01, -6.73240052e-01, -7.75536132e-01, -8.87068313e-01,
           -1.03562871e+00, -1.24528908e+00, -1.44636972e+00, -1.64381499e+00,
           -5.11405857e-01,  1.36036178e+00,  1.08392580e+00,  9.19487924e-01,
            8.92414226e-01,  6.81920253e-01,  6.39758108e-01,  1.02122441e+00,
            9.51617597e-01,  1.01340706e+00,  1.22154847e+00,  1.40592216e+00,
            2.00947401e+00,  7.05329292e-01,  1.63851130e+00,  1.68615760e+00,
           -1.52851275e-01, -2.18587976e-01, -2.63908376e-01, -3.60160489e-01,
           -5.37225095e-01, -7.11802199e-01, -8.26956438e-01, -9.51259205e-01,
           -1.12267557e+00, -1.30644784e+00, -1.62393964e+00, -1.76727431e+00,
           -1.17282733e+00,  1.22869245e+00,  1.13784171e+00,  1.04960243e+00,
            1.01726591e+00,  1.19561950e+00,  1.08212666e+00,  9.27143082e-01,
            9.87085466e-01,  1.09200748e+00,  1.31051280e+00,  1.33172162e+00,
            1.65489727e+00,  1.71982470e+00,  5.62404521e-01, -3.34838390e-01,
           -1.82333829e-01, -2.72657112e-01, -3.04250056e-01, -3.58705645e-01,
           -5.36566757e-01, -7.17045015e-01, -8.76772635e-01, -1.00089801e+00,
           -1.16662797e+00, -1.43474395e+00, -1.73831746e+00, -1.92223082e+00,
            1.18276161e+00,  1.08345355e+00,  9.85954468e-01,  9.52870539e-01,
            9.18412611e-01,  8.93670910e-01,  9.36209911e-01,  1.03562041e+00,
            9.98583450e-01,  1.09773455e+00,  1.25553737e+00,  1.29672191e+00,
            1.65257822e+00,  2.24006818e+00,  3.20731270e-01, -3.64384743e-01,
           -2.49634375e-01, -2.96257026e-01, -3.98518283e-01, -4.95652078e-01,
           -6.89694487e-01, -8.56112616e-01, -9.42418516e-01, -1.03561679e+00,
           -1.22250944e+00, -8.08656424e-01, -2.46176629e-03,  7.36283457e-01,
            1.04402891e+00,  8.15532044e-01,  9.17885505e-01,  9.18531446e-01,
            8.45367334e-01,  7.20719366e-01,  7.64364509e-01,  9.89953140e-01,
            1.10564860e+00,  1.09020317e+00,  1.13845183e+00,  1.27912972e+00,
            1.68504449e+00,  2.08605998e+00,  1.22149447e+00, -3.90510206e-01,
           -3.10451993e-01, -4.03139766e-01, -4.46566612e-01, -5.47776650e-01,
           -5.27361362e-01, -4.19794934e-01, -1.14064673e-01,  7.75216379e-02,
            8.78042092e-01,  1.13155165e+00,  9.52527142e-01,  9.60769339e-01,
            8.70854756e-01,  1.04469187e+00,  7.24383976e-01,  6.87407311e-01,
            7.28551957e-01,  1.03983060e+00,  9.41844722e-01,  1.17903694e+00,
            5.93210371e-01,  7.32252798e-01,  1.47107587e+00,  1.62976824e+00,
            1.77515992e+00,  2.02769894e+00,  1.86173196e+00, -4.10463637e-01,
           -3.22559353e-01,  5.32298530e-01,  2.46574332e+00,  2.30370038e+00,
            1.90359166e+00,  1.48590121e+00,  1.36596973e+00,  1.16661734e+00,
            1.02131006e+00,  9.28320313e-01,  7.71562399e-01,  7.57150705e-01,
            6.20292998e-01,  1.53828204e-01,  1.29583723e+00,  5.35447498e-01,
            6.64596661e-01,  9.50982277e-01,  1.39997034e+00,  1.46304795e+00,
            1.17268881e+00,  1.27562470e+00,  1.21130625e+00,  1.27389260e+00,
            1.68162004e+00,  2.04795973e+00,  2.33203355e+00, -4.07071204e-01,
           -1.79337259e-01,  3.06841190e+00,  3.05469347e+00,  2.49118117e+00,
            1.85092620e+00,  1.37930294e+00,  1.22744993e+00,  1.25568848e+00,
            1.02480139e+00,  8.12660995e-01,  7.36951191e-01,  9.20277865e-01,
            1.19339729e+00, -8.27211390e-01,  1.29096135e-01,  1.40473816e+00,
            1.01736472e+00,  9.35710820e-01,  5.24344078e-01,  2.94339878e-01,
            9.07487792e-01,  1.08482838e+00,  1.07410977e+00,  1.29557948e+00,
            1.76420818e+00,  2.08792065e+00,  2.19674138e+00, -3.96444792e-01,
            3.62801922e+00,  3.70084537e+00,  2.62758426e+00,  2.30240194e+00,
            1.86706191e+00,  1.59405364e+00,  1.44223559e+00,  1.52193171e+00,
            1.53922876e+00,  1.01594749e+00,  6.07517508e-01,  8.52154549e-01,
            8.73506843e-01,  1.25374468e+00, -8.66881681e-01, -9.50957943e-01,
           -6.43898584e-01, -4.47736099e-01,  3.19556706e-01,  1.09431249e+00,
            1.31073575e+00,  1.20255544e+00,  1.25543459e+00,  1.50035385e+00,
            1.84705681e+00,  2.13998084e+00,  2.43716472e+00,  6.08612376e-01,
            2.70575232e+00,  3.24252268e+00,  2.90110502e+00,  2.25285383e+00,
            1.54143689e+00,  1.35104865e+00,  1.27657777e+00,  1.46088761e+00,
            1.16634642e+00,  6.16089571e-01,  6.69720789e-01,  7.42053843e-01,
            6.44249610e-01,  9.20073321e-01,  1.27050944e+00,  6.71643371e-01,
            1.00719739e+00,  1.26300683e+00,  1.25145917e+00,  1.19504053e+00,
            1.36305960e+00,  1.26753744e+00,  1.24063196e+00,  1.57333860e+00,
            1.93855336e+00,  2.29317555e+00,  2.76951759e+00,  2.15908910e+00,
            1.75923435e+00,  3.40565740e+00,  2.56236942e+00,  2.18534461e+00,
            1.82276647e+00,  1.29431687e+00,  1.04355432e+00,  1.12718904e+00,
            9.40266136e-01,  7.32532341e-01,  7.59307348e-01,  8.53602404e-01,
            9.21250059e-01,  1.03117497e+00,  1.06331421e+00,  1.19362926e+00,
            1.01304386e+00,  9.53794126e-01,  8.53444308e-01,  1.07514108e+00,
            1.09401160e+00,  1.03378194e+00,  9.76145138e-01,  1.10104741e+00,
            1.57926107e+00,  2.26847927e+00,  2.68583832e+00,  4.20562813e+00,
           -2.18587697e-01,  2.19199720e+00,  3.47536690e+00,  2.34582324e+00,
            1.51130961e+00,  1.08750083e+00,  1.10126986e+00,  1.28692177e+00,
            1.10124653e+00,  9.87621203e-01,  9.36998806e-01,  8.10191831e-01,
            8.69012700e-01,  9.51706448e-01,  9.03233874e-01,  8.07958360e-01,
            7.31282451e-01,  7.45502161e-01,  8.95940821e-01,  9.86132489e-01,
            1.33141259e+00,  1.27620037e+00,  1.06272393e+00,  1.03023777e+00,
            1.55896254e+00,  2.13279914e+00,  3.17520398e+00,  3.95465060e+00,
           -1.85080533e-01, -2.63822628e-01,  1.11166741e+00,  2.51259872e+00,
            2.05084298e+00,  1.39829118e+00,  1.11645550e+00,  1.12606799e+00,
            8.69725672e-01,  7.51235723e-01,  6.92271401e-01,  6.59366113e-01,
            6.84595929e-01,  7.76352990e-01,  9.01243782e-01,  9.10217241e-01,
            9.17631528e-01,  1.01176759e+00,  1.13624376e+00,  1.26739400e+00,
            1.59895342e+00,  1.37267067e+00,  1.30155013e+00,  1.60031216e+00,
            2.09024699e+00,  3.07984093e+00,  2.99889432e+00, -1.87587669e-01,
            5.55242663e-03, -2.11476021e-01, -2.83460823e-01, -4.06901028e-01,
            2.91998975e-01,  1.61867385e+00,  1.77527398e+00,  2.03479180e+00,
            1.67569440e+00,  1.50255425e+00,  1.44492030e+00,  1.32896484e+00,
            1.35205854e+00,  1.21026076e+00,  1.28163434e+00,  9.51739381e-01,
            8.63854941e-01,  7.69097875e-01,  9.24032220e-01,  1.06994348e+00,
            1.38397017e+00,  1.38895773e+00,  1.17504585e+00,  1.42051157e+00,
            9.55662392e-01,  6.98519168e-01, -2.61527540e-01, -1.50491925e-01,
           -1.00006485e-01, -1.61635280e-01, -2.33852370e-01, -3.58257922e-01,
           -5.33278550e-01, -6.19899418e-01, -6.63315874e-01, -1.82564878e-01,
           -1.31175754e-01, -4.69446999e-01, -2.83334943e-01, -6.80896100e-01,
           -7.29320797e-01, -1.00401354e+00, -9.26076620e-01, -9.72110865e-01,
           -1.02542801e+00, -1.01118057e+00, -9.35905713e-01, -8.13999880e-01,
           -6.80882496e-01, -6.42959294e-01, -6.37148159e-01, -5.82556009e-01,
           -4.52872542e-01, -2.97427388e-01, -2.08924315e-01, -1.11372986e-01,
           -5.61756058e-02, -1.04372845e-01, -1.77726059e-01, -3.05511268e-01,
           -4.74838394e-01, -5.59138470e-01, -5.93877843e-01, -5.87841011e-01,
           -6.78360164e-01, -8.08294356e-01, -9.16538149e-01, -9.73307508e-01,
           -9.59816367e-01, -8.80617873e-01, -8.14345755e-01, -8.59919392e-01,
           -9.15808448e-01, -9.06842657e-01, -8.31536623e-01, -7.01824715e-01,
           -5.81968346e-01, -5.71359849e-01, -5.74336763e-01, -5.20471184e-01,
           -3.98166274e-01, -2.42559128e-01, -1.55186398e-01, -7.60371095e-02,
           -2.51591927e-02, -4.82334823e-02, -1.01549724e-01, -2.09119576e-01,
           -3.40462398e-01, -4.25818674e-01, -4.36428512e-01, -4.05461503e-01,
           -4.56937685e-01, -5.54523016e-01, -6.72072014e-01, -7.52476986e-01,
           -7.53908141e-01, -6.88920061e-01, -6.43264182e-01, -6.77526731e-01,
           -7.29744139e-01, -7.03057047e-01, -6.00521000e-01, -4.74798441e-01,
           -3.94259892e-01, -4.06094020e-01, -4.41358493e-01, -3.96626259e-01,
           -2.88155750e-01, -1.56811276e-01, -8.96730766e-02, -3.41472927e-02])




```python
X_train_scaled2 = X_train.reshape(-1,28*28) / 255
X_test_scaled2 = X_test.reshape(-1,28*28) / 255
```


```python
X_train_scaled2[0]
```




    array([0.        , 0.        , 0.        , 0.        , 0.        ,
           0.        , 0.        , 0.        , 0.        , 0.        ,
           0.        , 0.        , 0.        , 0.        , 0.        ,
           0.        , 0.        , 0.        , 0.        , 0.        ,
           0.        , 0.        , 0.        , 0.        , 0.        ,
           0.        , 0.        , 0.        , 0.        , 0.        ,
           0.        , 0.        , 0.        , 0.        , 0.        ,
           0.        , 0.        , 0.        , 0.        , 0.        ,
           0.        , 0.        , 0.        , 0.        , 0.        ,
           0.        , 0.        , 0.        , 0.        , 0.        ,
           0.        , 0.        , 0.        , 0.        , 0.        ,
           0.        , 0.        , 0.        , 0.        , 0.        ,
           0.        , 0.        , 0.        , 0.        , 0.        ,
           0.        , 0.        , 0.        , 0.        , 0.        ,
           0.        , 0.        , 0.        , 0.        , 0.        ,
           0.        , 0.        , 0.        , 0.        , 0.        ,
           0.        , 0.        , 0.        , 0.        , 0.        ,
           0.        , 0.        , 0.        , 0.        , 0.        ,
           0.        , 0.        , 0.        , 0.        , 0.        ,
           0.        , 0.00392157, 0.        , 0.        , 0.05098039,
           0.28627451, 0.        , 0.        , 0.00392157, 0.01568627,
           0.        , 0.        , 0.        , 0.        , 0.00392157,
           0.00392157, 0.        , 0.        , 0.        , 0.        ,
           0.        , 0.        , 0.        , 0.        , 0.        ,
           0.        , 0.        , 0.        , 0.        , 0.01176471,
           0.        , 0.14117647, 0.53333333, 0.49803922, 0.24313725,
           0.21176471, 0.        , 0.        , 0.        , 0.00392157,
           0.01176471, 0.01568627, 0.        , 0.        , 0.01176471,
           0.        , 0.        , 0.        , 0.        , 0.        ,
           0.        , 0.        , 0.        , 0.        , 0.        ,
           0.        , 0.        , 0.02352941, 0.        , 0.4       ,
           0.8       , 0.69019608, 0.5254902 , 0.56470588, 0.48235294,
           0.09019608, 0.        , 0.        , 0.        , 0.        ,
           0.04705882, 0.03921569, 0.        , 0.        , 0.        ,
           0.        , 0.        , 0.        , 0.        , 0.        ,
           0.        , 0.        , 0.        , 0.        , 0.        ,
           0.        , 0.        , 0.60784314, 0.9254902 , 0.81176471,
           0.69803922, 0.41960784, 0.61176471, 0.63137255, 0.42745098,
           0.25098039, 0.09019608, 0.30196078, 0.50980392, 0.28235294,
           0.05882353, 0.        , 0.        , 0.        , 0.        ,
           0.        , 0.        , 0.        , 0.        , 0.        ,
           0.        , 0.        , 0.00392157, 0.        , 0.27058824,
           0.81176471, 0.8745098 , 0.85490196, 0.84705882, 0.84705882,
           0.63921569, 0.49803922, 0.4745098 , 0.47843137, 0.57254902,
           0.55294118, 0.34509804, 0.6745098 , 0.25882353, 0.        ,
           0.        , 0.        , 0.        , 0.        , 0.        ,
           0.        , 0.        , 0.        , 0.00392157, 0.00392157,
           0.00392157, 0.        , 0.78431373, 0.90980392, 0.90980392,
           0.91372549, 0.89803922, 0.8745098 , 0.8745098 , 0.84313725,
           0.83529412, 0.64313725, 0.49803922, 0.48235294, 0.76862745,
           0.89803922, 0.        , 0.        , 0.        , 0.        ,
           0.        , 0.        , 0.        , 0.        , 0.        ,
           0.        , 0.        , 0.        , 0.        , 0.        ,
           0.71764706, 0.88235294, 0.84705882, 0.8745098 , 0.89411765,
           0.92156863, 0.89019608, 0.87843137, 0.87058824, 0.87843137,
           0.86666667, 0.8745098 , 0.96078431, 0.67843137, 0.        ,
           0.        , 0.        , 0.        , 0.        , 0.        ,
           0.        , 0.        , 0.        , 0.        , 0.        ,
           0.        , 0.        , 0.        , 0.75686275, 0.89411765,
           0.85490196, 0.83529412, 0.77647059, 0.70588235, 0.83137255,
           0.82352941, 0.82745098, 0.83529412, 0.8745098 , 0.8627451 ,
           0.95294118, 0.79215686, 0.        , 0.        , 0.        ,
           0.        , 0.        , 0.        , 0.        , 0.        ,
           0.        , 0.        , 0.00392157, 0.01176471, 0.        ,
           0.04705882, 0.85882353, 0.8627451 , 0.83137255, 0.85490196,
           0.75294118, 0.6627451 , 0.89019608, 0.81568627, 0.85490196,
           0.87843137, 0.83137255, 0.88627451, 0.77254902, 0.81960784,
           0.20392157, 0.        , 0.        , 0.        , 0.        ,
           0.        , 0.        , 0.        , 0.        , 0.        ,
           0.        , 0.02352941, 0.        , 0.38823529, 0.95686275,
           0.87058824, 0.8627451 , 0.85490196, 0.79607843, 0.77647059,
           0.86666667, 0.84313725, 0.83529412, 0.87058824, 0.8627451 ,
           0.96078431, 0.46666667, 0.65490196, 0.21960784, 0.        ,
           0.        , 0.        , 0.        , 0.        , 0.        ,
           0.        , 0.        , 0.        , 0.01568627, 0.        ,
           0.        , 0.21568627, 0.9254902 , 0.89411765, 0.90196078,
           0.89411765, 0.94117647, 0.90980392, 0.83529412, 0.85490196,
           0.8745098 , 0.91764706, 0.85098039, 0.85098039, 0.81960784,
           0.36078431, 0.        , 0.        , 0.        , 0.00392157,
           0.01568627, 0.02352941, 0.02745098, 0.00784314, 0.        ,
           0.        , 0.        , 0.        , 0.        , 0.92941176,
           0.88627451, 0.85098039, 0.8745098 , 0.87058824, 0.85882353,
           0.87058824, 0.86666667, 0.84705882, 0.8745098 , 0.89803922,
           0.84313725, 0.85490196, 1.        , 0.30196078, 0.        ,
           0.        , 0.01176471, 0.        , 0.        , 0.        ,
           0.        , 0.        , 0.        , 0.        , 0.24313725,
           0.56862745, 0.8       , 0.89411765, 0.81176471, 0.83529412,
           0.86666667, 0.85490196, 0.81568627, 0.82745098, 0.85490196,
           0.87843137, 0.8745098 , 0.85882353, 0.84313725, 0.87843137,
           0.95686275, 0.62352941, 0.        , 0.        , 0.        ,
           0.        , 0.        , 0.07058824, 0.17254902, 0.32156863,
           0.41960784, 0.74117647, 0.89411765, 0.8627451 , 0.87058824,
           0.85098039, 0.88627451, 0.78431373, 0.80392157, 0.82745098,
           0.90196078, 0.87843137, 0.91764706, 0.69019608, 0.7372549 ,
           0.98039216, 0.97254902, 0.91372549, 0.93333333, 0.84313725,
           0.        , 0.        , 0.22352941, 0.73333333, 0.81568627,
           0.87843137, 0.86666667, 0.87843137, 0.81568627, 0.8       ,
           0.83921569, 0.81568627, 0.81960784, 0.78431373, 0.62352941,
           0.96078431, 0.75686275, 0.80784314, 0.8745098 , 1.        ,
           1.        , 0.86666667, 0.91764706, 0.86666667, 0.82745098,
           0.8627451 , 0.90980392, 0.96470588, 0.        , 0.01176471,
           0.79215686, 0.89411765, 0.87843137, 0.86666667, 0.82745098,
           0.82745098, 0.83921569, 0.80392157, 0.80392157, 0.80392157,
           0.8627451 , 0.94117647, 0.31372549, 0.58823529, 1.        ,
           0.89803922, 0.86666667, 0.7372549 , 0.60392157, 0.74901961,
           0.82352941, 0.8       , 0.81960784, 0.87058824, 0.89411765,
           0.88235294, 0.        , 0.38431373, 0.91372549, 0.77647059,
           0.82352941, 0.87058824, 0.89803922, 0.89803922, 0.91764706,
           0.97647059, 0.8627451 , 0.76078431, 0.84313725, 0.85098039,
           0.94509804, 0.25490196, 0.28627451, 0.41568627, 0.45882353,
           0.65882353, 0.85882353, 0.86666667, 0.84313725, 0.85098039,
           0.8745098 , 0.8745098 , 0.87843137, 0.89803922, 0.11372549,
           0.29411765, 0.8       , 0.83137255, 0.8       , 0.75686275,
           0.80392157, 0.82745098, 0.88235294, 0.84705882, 0.7254902 ,
           0.77254902, 0.80784314, 0.77647059, 0.83529412, 0.94117647,
           0.76470588, 0.89019608, 0.96078431, 0.9372549 , 0.8745098 ,
           0.85490196, 0.83137255, 0.81960784, 0.87058824, 0.8627451 ,
           0.86666667, 0.90196078, 0.2627451 , 0.18823529, 0.79607843,
           0.71764706, 0.76078431, 0.83529412, 0.77254902, 0.7254902 ,
           0.74509804, 0.76078431, 0.75294118, 0.79215686, 0.83921569,
           0.85882353, 0.86666667, 0.8627451 , 0.9254902 , 0.88235294,
           0.84705882, 0.78039216, 0.80784314, 0.72941176, 0.70980392,
           0.69411765, 0.6745098 , 0.70980392, 0.80392157, 0.80784314,
           0.45098039, 0.        , 0.47843137, 0.85882353, 0.75686275,
           0.70196078, 0.67058824, 0.71764706, 0.76862745, 0.8       ,
           0.82352941, 0.83529412, 0.81176471, 0.82745098, 0.82352941,
           0.78431373, 0.76862745, 0.76078431, 0.74901961, 0.76470588,
           0.74901961, 0.77647059, 0.75294118, 0.69019608, 0.61176471,
           0.65490196, 0.69411765, 0.82352941, 0.36078431, 0.        ,
           0.        , 0.29019608, 0.74117647, 0.83137255, 0.74901961,
           0.68627451, 0.6745098 , 0.68627451, 0.70980392, 0.7254902 ,
           0.7372549 , 0.74117647, 0.7372549 , 0.75686275, 0.77647059,
           0.8       , 0.81960784, 0.82352941, 0.82352941, 0.82745098,
           0.7372549 , 0.7372549 , 0.76078431, 0.75294118, 0.84705882,
           0.66666667, 0.        , 0.00784314, 0.        , 0.        ,
           0.        , 0.25882353, 0.78431373, 0.87058824, 0.92941176,
           0.9372549 , 0.94901961, 0.96470588, 0.95294118, 0.95686275,
           0.86666667, 0.8627451 , 0.75686275, 0.74901961, 0.70196078,
           0.71372549, 0.71372549, 0.70980392, 0.69019608, 0.65098039,
           0.65882353, 0.38823529, 0.22745098, 0.        , 0.        ,
           0.        , 0.        , 0.        , 0.        , 0.        ,
           0.        , 0.        , 0.15686275, 0.23921569, 0.17254902,
           0.28235294, 0.16078431, 0.1372549 , 0.        , 0.        ,
           0.        , 0.        , 0.        , 0.        , 0.        ,
           0.        , 0.        , 0.        , 0.        , 0.        ,
           0.        , 0.        , 0.        , 0.        , 0.        ,
           0.        , 0.        , 0.        , 0.        , 0.        ,
           0.        , 0.        , 0.        , 0.        , 0.        ,
           0.        , 0.        , 0.        , 0.        , 0.        ,
           0.        , 0.        , 0.        , 0.        , 0.        ,
           0.        , 0.        , 0.        , 0.        , 0.        ,
           0.        , 0.        , 0.        , 0.        , 0.        ,
           0.        , 0.        , 0.        , 0.        , 0.        ,
           0.        , 0.        , 0.        , 0.        , 0.        ,
           0.        , 0.        , 0.        , 0.        , 0.        ,
           0.        , 0.        , 0.        , 0.        , 0.        ,
           0.        , 0.        , 0.        , 0.        ])




```python
X_train_scaled1.shape, X_train_scaled2.shape
```




    ((60000, 784), (60000, 784))



##### 결과
- 784개의 픽셀로 이루어진 사진 데이터가 60000개 준비됨


```python
from sklearn.model_selection import cross_validate
from sklearn.linear_model import SGDClassifier

train_scores = []
test_scores = []

max_iter_list = [1,3,5,7,9,10]

for max_iter in max_iter_list:
    sc = SGDClassifier(loss='log_loss',max_iter=max_iter,random_state=42)
    scores = cross_validate(sc,X_train_scaled1,y_train,return_train_score=True,n_jobs=1)
    test_scores.append(np.mean(scores['test_score']))
    train_scores.append(np.mean(scores['train_score']))
```

    /usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.
      warnings.warn(
    /usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.
      warnings.warn(
    /usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.
      warnings.warn(
    /usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.
      warnings.warn(
    /usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.
      warnings.warn(
    /usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.
      warnings.warn(
    /usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.
      warnings.warn(
    /usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.
      warnings.warn(
    /usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.
      warnings.warn(
    /usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.
      warnings.warn(
    /usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.
      warnings.warn(
    /usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.
      warnings.warn(
    /usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.
      warnings.warn(
    /usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.
      warnings.warn(
    /usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.
      warnings.warn(
    /usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.
      warnings.warn(
    /usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.
      warnings.warn(
    /usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.
      warnings.warn(
    /usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.
      warnings.warn(
    /usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.
      warnings.warn(
    /usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.
      warnings.warn(
    /usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.
      warnings.warn(
    /usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.
      warnings.warn(
    /usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.
      warnings.warn(
    /usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.
      warnings.warn(
    /usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.
      warnings.warn(
    /usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.
      warnings.warn(
    /usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.
      warnings.warn(
    /usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.
      warnings.warn(
    /usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.
      warnings.warn(
    


```python
test_scores
```




    [0.8251166666666666,
     0.8308666666666668,
     0.8336833333333334,
     0.8318000000000001,
     0.8338166666666667,
     0.8368]




```python
import matplotlib.pyplot as plt

plt.figure(figsize=(10,5))
plt.plot(max_iter_list,train_scores,label='train_score',marker='o')
plt.plot(max_iter_list,test_scores,label='test_score',marker='o')
plt.xlabel('max_iter')
plt.ylabel('score')
plt.title('Train and Test Scores vs Max Iterations')
plt.legend()
plt.grid(True)
plt.show()
```


    
![png](5_01_%EC%9D%B8%EA%B3%B5%EC%8B%A0%EA%B2%BD%EB%A7%9D_%EA%B0%9C%EC%9D%B8%ED%95%99%EC%8A%B5_files/5_01_%EC%9D%B8%EA%B3%B5%EC%8B%A0%EA%B2%BD%EB%A7%9D_%EA%B0%9C%EC%9D%B8%ED%95%99%EC%8A%B5_21_0.png)
    


##### 결과
- SGDClassifier의 max_iter값을 조정하여 반복 횟수를 증가시켜도 성능 향상폭이 크지 않음

##### 성능 향상을 위한 고민
- 선형 회귀와 동일한 선형 방정식으로 학습  
    - $z = a \times (weight) + b \times (length) + c \times (diagonal) + d \times (deight) + e \times (width) + f$
- 패션 데이터에 맞추어 변경하면
    - $z\_티셔츠 = w1 \times (픽셀1) + w2 \times (픽셀2)\ +\ ...\ + w784 \times (픽셀784) + b$
    - 총 784개의 픽셀이므로 길이가 긴 식이 만들어짐
- 바지 데이터에 대한 방정식은
    - $z\_바지 = w1' \times (픽셀1) + w2' \times (픽셀2)\ +\ ...\ + w784' \times (픽셀784) + b'$
    - 티셔츠에 대한 방정식과 비슷하지만 가중치와 절편은 다른 값을 사용(그렇지 않으면 바지와 티셔츠 구분 불가)
- 위와 같은 방식으로 나머지 클래스에 대한 방정식이 만들어진다고 이해할 수 있음
- SGDClassifier 모델은 MNIST데이터의 클래스를 구분하기 위해 10개의 방정식에 대한 모델 파라미터(가중치와 절편)를 찾음

- 방정식을 계산한 후 소프트맥스 함수를 통과하여 각 클래스에 대한 확률을 얻음

# 인공신경망
- 1943년 워런 매컬러와 월터 피츠가 제안한 뉴런 모델(매컬러-피츠 뉴런)
- 다음 그림과 같은 인간의 뇌 동작을 표현하기 위해 만들어짐
![image-2.png](attachment:image-2.png)

## 생물학적 뉴런의 동작
- 입력은 수상돌기를 통해 들어옴(다수)
- 축색(축삭)을 통해 출력으로 내보낸다(단수)
- 다른 뉴런으로부터 전달받은 입력은 수상돌기로 연결되어 덧셈을 진행
- 입력은 합산되어 출력통로인 축색으로 출력(0 or 1)
- 합친 값이 어떤 설정치(임계값) 이상이면 출력되고 그 이하인 경우 출력 없음(역치값. threshold)
- 뉴런과 뉴런은 시냅스로 연결되어 있음
- 연결된 다음 뉴런을 억제할 것인지 흥분시킬 것인지 두 가지 동작을 함
- 시냅스의 전기적 신호는 이온을 사용

## 인공 신경망 활용
- 이미지 분류 문제에는 인공 신경망(딥러닝)이 알맞은 경우가 많음
- 가장 기본적인 인공 신경망의 형태는 확률적 경사하강법을 사용하는 로지스틱 회귀를 기반으로 함
- 패션 아이템 분류 문제를 인공 신경망으로 표현하면 다음과 같음
![image-3.png](attachment:image-3.png)
- 784개의 픽셀 하나 하나($x_1$ ~ $x_{784}$)를 나타내는 입력층
- 총 10개의 클래스를 예측($z_1$ ~ $z_{10}$)하는 출력층
- z값을 계산하는 단위를 표현하는 용어(뉴런, 유닛)
- 각 픽셀에 곱해지는 가중치 $w_{1.1}$, $w_{1.2} ...$
- 절편은 뉴런마다 하나씩 $b_1$ ~ $b_{10}$으로 나타냄

### 생물학적 뉴런과 인공 신경망의 차이
- 생물학적 뉴런은 가중치와 입력을 곱하여 출력을 만드는 동작을 하지는 않음(소프트맥스 X, 시그모이드 X)
- 인공 신경망은 생물학적 뉴런의 모양을 본뜬 수학적 모델임(실제 생물학적 뉴런의 동작은 아직 알 수 없는 부분이 많음)
- 인공 신경망이나 AI라는 개념이 실제 사람의 뇌와 같다는 생각은 잘못된 것
- 기존 머신러닝 알고리즘으로 해결하기 어려웠던 문제를 해결하기 위한 새로운 머신러닝 알고리즘 개념

## 텐서플로우와 케라스
### 텐서플로(Tensorflow)
- 구글이 2015년 11월 오픈소스로 공개한 딥러닝 라이브러리
- 코랩을 이용하는 경우 바로 사용 가능
- 직접 사용하려는 경우 설치 필요
- 딥러닝 라이브러리는 GPU사용 가능(GPU는 벡터와 행렬 연산에 최적화된 장치)
- 텐서플로는 저수준 API와 고수준 API(Keras) 지원

### 케라스(Keras)
- 직접 GPU연산을 하지 않고 GPU연산을 수행하는 백엔드 라이브러리(텐서플로, 씨아노, CNTK 등)를 사용
- 멀티-백엔드 케라스라고 부르며 이는 케라스를 통해 다양한 백엔드 딥러닝 라이브러리를 사용할 수 있음을 의미
- 직관적으로 사용하기 편한 고수준 API를 제공
- 현재는 텐서플로우 라이브러리에 케라스 API가 내장됨(tensorflow 2.0+)
- 텐서플로우의 핵심 API이며 멀티-백엔드 케라스는 2.3.1이루 개발 중지됨(텐서플로 == 케라스)

### 인공 신경망으로 모델 만들기
#### 텐서플로에서 케라스 사용을 위한 임포트 선언


```python
import tensorflow as tf
from tensorflow import keras
```

#### 데이터 준비
- 교차검증을 사용하지 않는 이유
    - 딥러닝 분야의 데이터셋은 데이터 양이 충분하기 때문에 검증 점수가 안정적임
    - 데이터 양이 많으므로 훈련 시간이 많이 필요함(몇 시간 내지 며칠 등의 시간이 필요할 수 있음)


```python
from tensorflow import keras

(X_train, y_train), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()

X_train_scaled = X_train / 255.0
X_train_scaled = X_train_scaled.reshape(-1,28*28)

from sklearn.model_selection import train_test_split

X_train_scaled, X_test_val_scaled, y_train, y_test_val = train_test_split(X_train_scaled, y_train, test_size=0.2, random_state=42)
```

    Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz
    29515/29515 [==============================] - 0s 0us/step
    Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz
    26421880/26421880 [==============================] - 0s 0us/step
    Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz
    5148/5148 [==============================] - 0s 0us/step
    Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz
    4422102/4422102 [==============================] - 0s 0us/step
    

#### 훈련 세트 크기 확인


```python
X_train_scaled.shape, y_train.shape
```




    ((48000, 784), (48000,))



#### 검증 세트 크기 확인


```python
X_test_val_scaled.shape, y_test_val.shape
```




    ((12000, 784), (12000,))



#### 인공 신경망에서 오른쪽 출력층 생성
![image-4.png](attachment:image-4.png)
- 케라스의 레이어 패키지 사용
- 밀집층(dense layer)이 가장 기본이 됨
![image-5.png](attachment:image-5.png)
- 위와 같이 연결된 형태를 밀집이라고 표현
- 모든 뉴런이 연결하고 있으므로 완전 연결층(fully connected layer)이라고 부르기도 함
- 케라스의 Dense 클래스를 이용하여 밀집층 생성
    - units=10 :뉴런의 개수 매개변수
    - activation='softmax' :뉴런의 출력에 적용할 함수(이진 분류라면 'sigmoid')
    - input_shape=(784,) :입력의 크기

#### 밀집(dense)층 생성


```python
from tensorflow.keras.layers import Dense

dense = Dense(10, activation='softmax')
```


```python
from tensorflow.keras.models import Sequential

model = keras.Sequential()
```


```python
from tensorflow.keras.layers import Input

input_layer = Input(shape=(784,))
model.add(input_layer)
```

#### 밀집층을 가진 신경망 모델 생성
- Sequantial 클래스로 객체 생성
- 만들어진 Sequential객체는 신경망 모델
- 모델에 layer(입력-은닉-출력)을 추가하여 신경망 모델을 구성


```python
model.add(dense)
```

#### 만들어진 신경망 모델 개념
![image.png](attachment:image.png)
- 입력층과 출력층 사이에 연결선을 제외하고 가중치와 절편은 표시하지 않음
- 그림으로 표현하지는 않았지만 절편이 뉴런마다 더해진다는 것은 기본 개념
- 뉴런의 선형 방정식 결과에 적용되는 소프트맥스와 같은 함수를 활성화 함수(activation function)라고 함
- 시그모이드 또는 소프트맥스 함수는 뉴런의 출력에 바로 적용되므로 층의 일부로 나타내기도 함

# 인공 신경망으로 패션 아이템 분류
- 사이킷런과 비교해 케라스에서 모델을 만드는 방식은 약간의 차이가 있음

## 케라스 모델 훈련 전 설정 단계
- model객체의 compile() 메서드 사용
- 손실 함수의 종류 지정은 필수
- 훈련 과정에서 계산하고자 하는 측정값을 지정


```python
model.compile(loss='sparse_categorical_crossentropy', metrics=['accuracy'])
```

#### loss
- 이진분류는 이진 크로스 엔트로피 손실 함수 사용
    - 케라스에서 이진 분류 시 손실 함수: loss='binary_crossentropy'
- 다중 분류에서는 크로스 엔트로피 손실 함수 사용
    - 케라스에서 다중 분류 시 손실 함수: loss='categorical_crossentropy'
- sparse: 희소
- 텐서플로우에서는 타겟값을 원-핫 인코딩으로 바꾸지 않아도 바로 사용 가능
    - 정수로 된 타겟값을 이용하여 크로스 엔트로피 손실을 계산하는 경우
    - loss='sparse_categorical_crossentropy'
    - 데이터의 타겟값이 원-핫 인코딩으로 되어 있다면 categorical_crossentropy로 지정


```python
y_train[:10]
```




    array([7, 3, 5, 8, 6, 9, 3, 3, 9, 9], dtype=uint8)



- 타겟값이 정수형이므로 sparse_categorical_crossentropy로 지정함

#### metrics
- 케라스는 모델이 훈련할 때 기본적으로 에포크마다 손실값을 출력
- 손실의 변화를 보면서 훈련이 잘되는 것인지 정확도를 출력할 수 있도록 지표를 지정할 때 사용
- 분류이므로 정확도 지표(accuracy)를 지정

#### 모델 훈련
- fit()메서드는 사이킷런과 유사함
- 처음 두 개의 매개변수에 입력데이터와 정답데이터를 지정
- epochs 매개변수로 반복할 에포크 횟수 지정


```python
model.fit(X_train_scaled, y_train, epochs=5)
```

    Epoch 1/5
    1500/1500 [==============================] - 6s 3ms/step - loss: 0.6063 - accuracy: 0.7962
    Epoch 2/5
    1500/1500 [==============================] - 9s 6ms/step - loss: 0.4739 - accuracy: 0.8388
    Epoch 3/5
    1500/1500 [==============================] - 6s 4ms/step - loss: 0.4501 - accuracy: 0.8480
    Epoch 4/5
    1500/1500 [==============================] - 4s 2ms/step - loss: 0.4375 - accuracy: 0.8529
    Epoch 5/5
    1500/1500 [==============================] - 4s 3ms/step - loss: 0.4291 - accuracy: 0.8549
    




    <keras.src.callbacks.History at 0x7d7f8e6248b0>



##### 결과
- 텐서플로는 랜덤하게 동작하는 특성이 있으므로 실행마다 약간씩 결과가 달라짐
- 모델 컴파일 시 지정한 metric에 따라 각 에포크 마다 결과를 보여줌

#### 모델 성능 평가
- 케라스에서 모델의 성능을 평가하는 메서드를 이용
- evaluate()


```python
model.evaluate(X_test_val_scaled, y_test_val)
```

    375/375 [==============================] - 1s 2ms/step - loss: 0.4406 - accuracy: 0.8505
    




    [0.44064074754714966, 0.8504999876022339]



##### 결과
- 검증세트의 점수는 훈련세트 점수보다 조금 낮은 것이 일반적임
- 훈련세트 점수
    - accuracy: 0.8625
    - accuracy: 0.8637
    - accuracy: 0.8635
    - accuracy: 0.8652
    - accuracy: 0.8660
- 검증세트 점수
    - accuracy: 0.8517


```python
X_train_scaled[3]
```




    array([0.        , 0.        , 0.        , 0.        , 0.        ,
           0.        , 0.        , 0.        , 0.        , 0.01176471,
           0.        , 0.        , 0.24313725, 0.36862745, 0.37647059,
           0.3254902 , 0.        , 0.        , 0.01176471, 0.        ,
           0.        , 0.        , 0.        , 0.        , 0.        ,
           0.        , 0.        , 0.        , 0.        , 0.        ,
           0.        , 0.        , 0.        , 0.        , 0.        ,
           0.        , 0.01176471, 0.        , 0.        , 0.64705882,
           0.60784314, 0.18039216, 0.15686275, 0.33333333, 0.72156863,
           0.3372549 , 0.        , 0.00392157, 0.        , 0.        ,
           0.        , 0.        , 0.        , 0.        , 0.        ,
           0.        , 0.        , 0.        , 0.        , 0.        ,
           0.        , 0.        , 0.        , 0.00392157, 0.        ,
           0.        , 0.74509804, 0.37647059, 0.        , 0.        ,
           0.        , 0.        , 0.        , 0.54509804, 0.36470588,
           0.        , 0.00392157, 0.        , 0.        , 0.        ,
           0.        , 0.        , 0.        , 0.        , 0.        ,
           0.        , 0.        , 0.        , 0.        , 0.        ,
           0.        , 0.01568627, 0.        , 0.55686275, 0.5372549 ,
           0.        , 0.03529412, 0.01568627, 0.01176471, 0.03137255,
           0.        , 0.        , 0.67058824, 0.07843137, 0.        ,
           0.01176471, 0.        , 0.00392157, 0.        , 0.        ,
           0.        , 0.        , 0.        , 0.        , 0.        ,
           0.        , 0.        , 0.        , 0.00392157, 0.        ,
           0.        , 0.74509804, 0.03137255, 0.        , 0.01176471,
           0.        , 0.        , 0.        , 0.03137255, 0.        ,
           0.39607843, 0.51372549, 0.        , 0.02352941, 0.        ,
           0.        , 0.        , 0.        , 0.        , 0.        ,
           0.        , 0.        , 0.        , 0.        , 0.        ,
           0.        , 0.01568627, 0.        , 0.2627451 , 0.68235294,
           0.        , 0.        , 0.        , 0.        , 0.        ,
           0.        , 0.00392157, 0.        , 0.05490196, 0.64705882,
           0.        , 0.        , 0.        , 0.        , 0.        ,
           0.        , 0.        , 0.        , 0.        , 0.        ,
           0.        , 0.        , 0.        , 0.        , 0.01568627,
           0.        , 0.36470588, 0.58823529, 0.        , 0.01176471,
           0.        , 0.        , 0.        , 0.        , 0.        ,
           0.        , 0.        , 0.66666667, 0.01176471, 0.        ,
           0.00392157, 0.00392157, 0.        , 0.        , 0.        ,
           0.        , 0.        , 0.        , 0.        , 0.        ,
           0.        , 0.        , 0.01568627, 0.        , 0.40784314,
           0.49019608, 0.        , 0.02352941, 0.        , 0.        ,
           0.        , 0.        , 0.        , 0.        , 0.        ,
           0.68235294, 0.10980392, 0.        , 0.01176471, 0.        ,
           0.        , 0.        , 0.        , 0.        , 0.        ,
           0.        , 0.        , 0.        , 0.        , 0.        ,
           0.01176471, 0.        , 0.4       , 0.45882353, 0.        ,
           0.02352941, 0.        , 0.        , 0.        , 0.        ,
           0.        , 0.01568627, 0.        , 0.65882353, 0.15686275,
           0.        , 0.02352941, 0.        , 0.        , 0.        ,
           0.        , 0.        , 0.        , 0.        , 0.        ,
           0.        , 0.        , 0.        , 0.01568627, 0.        ,
           0.42745098, 0.58431373, 0.        , 0.02352941, 0.        ,
           0.        , 0.        , 0.        , 0.        , 0.        ,
           0.        , 0.65882353, 0.15686275, 0.        , 0.02352941,
           0.        , 0.        , 0.        , 0.        , 0.        ,
           0.        , 0.        , 0.        , 0.        , 0.        ,
           0.        , 0.01568627, 0.        , 0.38823529, 0.58431373,
           0.        , 0.        , 0.        , 0.        , 0.        ,
           0.        , 0.        , 0.        , 0.        , 0.65098039,
           0.26666667, 0.        , 0.        , 0.00392157, 0.        ,
           0.        , 0.        , 0.        , 0.        , 0.        ,
           0.        , 0.        , 0.        , 0.01176471, 0.        ,
           0.50588235, 0.70196078, 0.43921569, 0.        , 0.08627451,
           0.        , 0.        , 0.        , 0.        , 0.        ,
           0.2       , 0.00392157, 0.58823529, 0.67058824, 0.34901961,
           0.        , 0.        , 0.        , 0.        , 0.        ,
           0.        , 0.        , 0.        , 0.        , 0.        ,
           0.00392157, 0.        , 0.        , 0.79215686, 0.64705882,
           0.55294118, 0.36470588, 0.36470588, 0.3254902 , 0.2627451 ,
           0.20392157, 0.23529412, 0.36470588, 0.72941176, 0.55686275,
           0.61960784, 0.69019608, 0.70980392, 0.        , 0.        ,
           0.00392157, 0.        , 0.        , 0.        , 0.        ,
           0.        , 0.        , 0.        , 0.01176471, 0.        ,
           0.04705882, 0.76078431, 0.30588235, 0.4627451 , 0.25490196,
           0.12941176, 0.26666667, 0.55294118, 0.53333333, 0.59607843,
           0.39607843, 0.2627451 , 0.37647059, 0.61568627, 0.47058824,
           0.60784314, 0.0745098 , 0.        , 0.00392157, 0.        ,
           0.        , 0.        , 0.        , 0.        , 0.        ,
           0.        , 0.01176471, 0.        , 0.28627451, 0.74509804,
           0.35686275, 0.57647059, 0.29411765, 0.15686275, 0.18823529,
           0.18823529, 0.05490196, 0.09803922, 0.18823529, 0.23529412,
           0.6       , 0.68235294, 0.50196078, 0.67843137, 0.16862745,
           0.        , 0.        , 0.        , 0.        , 0.        ,
           0.        , 0.        , 0.        , 0.        , 0.        ,
           0.        , 0.5372549 , 0.76078431, 0.74509804, 0.89803922,
           0.21176471, 0.6       , 0.33333333, 0.07843137, 0.        ,
           0.        , 0.24313725, 0.        , 0.23529412, 0.72156863,
           0.80392157, 0.83921569, 0.35686275, 0.        , 0.        ,
           0.        , 0.        , 0.        , 0.        , 0.        ,
           0.        , 0.        , 0.        , 0.        , 0.68235294,
           0.70980392, 0.61960784, 0.60784314, 0.21176471, 0.39607843,
           0.15686275, 0.17254902, 0.        , 0.        , 0.44313725,
           0.1254902 , 0.01568627, 0.50196078, 0.6       , 0.63921569,
           0.61568627, 0.        , 0.        , 0.        , 0.        ,
           0.        , 0.        , 0.        , 0.        , 0.        ,
           0.        , 0.        , 0.69019608, 0.71372549, 0.67058824,
           0.71372549, 0.41960784, 0.00392157, 0.09411765, 0.28235294,
           0.00392157, 0.        , 0.30588235, 0.31372549, 0.79215686,
           0.58823529, 0.72941176, 0.65882353, 0.69019608, 0.07843137,
           0.        , 0.        , 0.        , 0.        , 0.        ,
           0.        , 0.        , 0.        , 0.        , 0.33333333,
           0.83921569, 0.65882353, 0.69019608, 0.69803922, 0.52156863,
           0.3372549 , 0.2627451 , 0.53333333, 0.        , 0.25098039,
           0.50588235, 0.37647059, 0.56470588, 0.55686275, 0.76470588,
           0.71372549, 0.67058824, 0.33333333, 0.        , 0.        ,
           0.        , 0.        , 0.        , 0.        , 0.        ,
           0.        , 0.        , 0.74117647, 0.82352941, 0.71372549,
           0.75294118, 0.76470588, 0.53333333, 0.62745098, 0.58431373,
           0.56862745, 0.41960784, 0.56862745, 0.71372549, 0.63137255,
           0.67058824, 0.63137255, 0.76078431, 0.67058824, 0.70196078,
           0.64705882, 0.        , 0.        , 0.        , 0.        ,
           0.        , 0.        , 0.        , 0.        , 0.        ,
           0.98039216, 0.79607843, 0.76078431, 0.77254902, 0.72941176,
           0.50588235, 0.6       , 0.58823529, 0.67843137, 0.79215686,
           0.60784314, 0.70980392, 0.79607843, 0.73333333, 0.74509804,
           0.90196078, 0.75294118, 0.57647059, 0.85490196, 0.03137255,
           0.        , 0.        , 0.        , 0.        , 0.        ,
           0.        , 0.        , 0.40784314, 0.94901961, 0.76470588,
           0.66666667, 0.70196078, 0.72156863, 0.50196078, 0.55294118,
           0.56862745, 0.83529412, 0.87843137, 0.87843137, 0.72941176,
           0.71372549, 0.67843137, 0.72941176, 0.92156863, 0.81568627,
           0.5254902 , 0.80392157, 0.34901961, 0.        , 0.        ,
           0.        , 0.        , 0.        , 0.        , 0.        ,
           0.81568627, 1.        , 0.76078431, 0.67843137, 0.86666667,
           0.78431373, 0.50588235, 0.65098039, 0.63921569, 0.83529412,
           0.89803922, 0.87058824, 0.74117647, 0.74117647, 0.74117647,
           0.70980392, 0.91764706, 0.82352941, 0.69019608, 0.83529412,
           0.41176471, 0.        , 0.        , 0.        , 0.        ,
           0.        , 0.        , 0.        , 0.64705882, 0.94901961,
           0.81568627, 0.88627451, 0.87843137, 0.76470588, 0.53333333,
           0.69803922, 0.50588235, 0.79215686, 0.88627451, 0.82352941,
           0.71372549, 0.74117647, 0.80392157, 0.75294118, 0.90980392,
           0.83921569, 0.70196078, 0.85490196, 0.2627451 , 0.        ,
           0.        , 0.        , 0.        , 0.        , 0.        ,
           0.        , 0.09411765, 0.92156863, 0.76470588, 0.74117647,
           0.84705882, 0.71372549, 0.56862745, 0.6       , 0.56470588,
           0.75294118, 0.85490196, 0.84705882, 0.75294118, 0.73333333,
           0.74509804, 0.73333333, 0.82745098, 0.82352941, 0.67843137,
           0.79215686, 0.        , 0.        , 0.        , 0.        ,
           0.        , 0.        , 0.        , 0.        , 0.        ,
           0.87058824, 0.77647059, 0.69019608, 0.73333333, 0.67058824,
           0.52156863, 0.61568627, 0.65098039, 0.83921569, 0.98039216,
           0.83529412, 0.80392157, 0.67058824, 0.76470588, 0.76470588,
           0.83529412, 0.80392157, 0.76078431, 0.62745098, 0.        ,
           0.        , 0.        , 0.        , 0.        , 0.        ,
           0.        , 0.        , 0.        , 0.23137255, 0.98039216,
           0.80784314, 0.88627451, 0.85882353, 0.48235294, 0.56862745,
           0.61960784, 0.58823529, 0.39607843, 0.3372549 , 0.54509804,
           0.67058824, 0.56470588, 0.4745098 , 0.90980392, 0.82745098,
           0.86666667, 0.31764706, 0.        , 0.        , 0.        ,
           0.        , 0.        , 0.        , 0.        , 0.        ,
           0.        , 0.        , 0.40784314, 0.67843137, 0.61568627,
           0.63921569, 0.55686275, 0.57647059, 0.57647059, 0.50588235,
           0.50196078, 0.47058824, 0.49411765, 0.55294118, 0.50588235,
           0.50196078, 0.57647059, 0.65882353, 0.50588235, 0.        ,
           0.        , 0.        , 0.        , 0.        ])




```python
# 훈련데이터로 예측해보기
for i in range(10):
    y_pred = model.predict(X_train_scaled[i:i+1])
    print(f'정답:{y_train[i]}', end=', ')
    for j in range(10):
        if y_pred[0][j] ==y_pred.max():
            print(f'예측:[{j}]')
```

    1/1 [==============================] - 0s 306ms/step
    정답:7, 예측:[7]
    1/1 [==============================] - 0s 86ms/step
    정답:3, 예측:[1]
    1/1 [==============================] - 0s 142ms/step
    정답:5, 예측:[5]
    1/1 [==============================] - 0s 124ms/step
    정답:8, 예측:[8]
    1/1 [==============================] - 0s 48ms/step
    정답:6, 예측:[2]
    1/1 [==============================] - 0s 85ms/step
    정답:9, 예측:[9]
    1/1 [==============================] - 0s 34ms/step
    정답:3, 예측:[3]
    1/1 [==============================] - 0s 36ms/step
    정답:3, 예측:[3]
    1/1 [==============================] - 0s 35ms/step
    정답:9, 예측:[9]
    1/1 [==============================] - 0s 38ms/step
    정답:9, 예측:[9]
    


```python
y_pred = model.predict(X_train_scaled[0:1])
```

    1/1 [==============================] - 0s 123ms/step
    


```python
X_train_scaled[0].shape
```




    (784,)




```python
X_train_scaled[0:1].shape
```




    (1, 784)




```python
y_pred
```




    array([[5.95788151e-06, 1.08290955e-07, 1.29684604e-05, 4.27659508e-09,
            1.22855181e-05, 1.66811198e-02, 4.52385022e-04, 3.98580305e-05,
            3.49184033e-03, 9.79303479e-01]], dtype=float32)


